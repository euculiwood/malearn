{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一章 统计学习及监督学习盖伦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "统计学习的分类\n",
    "\n",
    "基本分类\n",
    "      1 监督学习\n",
    "        从已标注的数据中学习预测模型\n",
    "      2 无监督学习\n",
    "        从未标注的数据中学习预测模型\n",
    "      3 强化学习\n",
    "        智能系统在与环境的连续互动中学习最优行为策略\n",
    "      4 半监督学习与主动学习\n",
    "        利用标注数据和未标注数据学习预测模型，少量标注数据，大量未标注数据\n",
    "        主动学习是系统主动给出实例让人进行标注，然后利用该标注数据学习预测模型\n",
    "        更接近监督学习\n",
    "     \n",
    "     \n",
    "按模型分类\n",
    "      1 概率模型和非概率模型/确定学习\n",
    "        监督学习中\n",
    "        概率模型取条件概率分布形式  P（y|x）\n",
    "        非概率模型取函数形式 y = f（x）\n",
    "        x输入，y输出 \n",
    "         \n",
    "        非监督学习中\n",
    "        概率模型取条件概率分布形式 p（z|x）或 p（x|z）\n",
    "        非概率模型取函数形式 z = g（x）\n",
    "        x输入，z输出\n",
    "        \n",
    "        条件概率最大化后得到函数，函数归一化后得到条件概率分布\n",
    "      \n",
    "      2 线性模型和非线性模型 \n",
    "        特别是非概率模型，可分为线性模型和非线性模型，指函数y=f（x）或z=g（x）为线性函数\n",
    "        \n",
    "      3 参数化模型和非参数化模型\n",
    "        参数化模型: 假设模型参数固定，可由有限维的参数完全刻画 \n",
    "        非参数化模型：假设模型参数的维度不固定或无穷大（随训练数据量增大）\n",
    "        \n",
    "按算法分类\n",
    "      在线学习（online learning）\n",
    "        每次接受一个样本进行预测，然后学习模型，重复\n",
    "      批量学习（batch learning）\n",
    "        一次接受所有数据，学习模型，进行预测 \n",
    "        \n",
    "按技巧分类\n",
    "      贝叶斯学习\n",
    "        用贝叶斯定理计算在给定数据条件下模型的条件概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(x|D) = \\frac{P(\\theta)P(D|\\theta)}{P(D)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "𝑃(𝜃)为先验概率，𝑃(D|𝜃)为似然函数\n",
    "      核方法（kernel method）\n",
    "         把线性模型扩展到非线性模型，一般显式的定义从输入空间到特征空间的映射，然后在特征空间中进行内积计算。\n",
    "         核方法直接定义核函数（映射之后在特征空间的内积），简化计算。\n",
    "         \n",
    "  \n",
    "统计学习三要素\n",
    "                模型（假设）          策略（学习）          算法（决策）   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  假设空间用Ｆ表示，假设空间可定义为决策函数的集合\n",
    "        $$  F = \\{f|Y = f(X) \\}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带参数向量的函数族\n",
    "$$ F = \\{f|Y = f_{\\theta}(X),\\theta \\in R^n \\}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件概率集合\n",
    "$$  F = \\{P|P(Y|X)\\}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带参数向量\n",
    "$$  F=\\{P|P_{\\theta}(Y|X),\\theta \\in R^n \\}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.损失函数和风险函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  用一个损失函数（loss function）或代价函数（cost function）度量错误程度\n",
    "  记做L（Y，f（X））"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）0-1损失函数（0-1 loss function）\n",
    "    $$ L(Y|f(X))=\\begin{cases}\n",
    "    1,&Y\\neq f(x)\\\\0,&Y=f(X)\n",
    "    \\end{cases}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)平方损失函数（quadratic loss function）\n",
    "$$  L(Y|f(X)) = (Y-f(X))^2  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)绝对损失函数\n",
    "$$  L(Y,f(X)) = |Y-f(X)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4）对数损失函数\n",
    "$$  L(Y,P(Y|X) = -\\log P(Y|X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数值越小，模型越好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数的期望是\n",
    "$$ R_{exp}(f) = E_p[L(Y,f(X)]= \\int_{x\\times y} L(y,f(x))P(x,y){\\rm d}x{\\rm d}y  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是f(x)关于联合分布P（X，Y）的平均意义上的损失，称为风险函数（risk function）或期望损失（expected loss），学习目标就是期望风险更小 的模型。\n",
    "给定一个训练数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  T=\\{(x_1,y_1),(x_2,y_2),\\cdot \\cdot \\cdot,(x_N,y_N) \\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型f（x）关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失（empirical loss），记做$ R_{emp}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ R_{emp}(f) = \\frac{1}{N}\\sum_{i=1}^{N}{L(y_i,f(x_i))}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R_{exp}（f）$是模型关于联合分布的期望损失，$R_{emp}(f)$是模型关于训练样本集的平均损失。\n",
    "当样本容量N趋于无穷时，后者趋于前者。\n",
    "但由于一般训练样本有限，需要对经验风险进行一定的矫正。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "经验风险最小化，结构风险最小化\n",
    "\n",
    "在假设空间、损失函数和训练数据集确定的情况下，可以确定经验风险函数。\n",
    "经验风险最小化，认为经验风险最小的模型是最优化模型，可以按照该模型求解最优化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即解决\n",
    "    $$  \\min_{f\\in F} \\frac{1}{N}\\sum_{i=1}^{N}{L(y_i,f(x_i))} $$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "极大似然估计就是经验风险最小化，模型为条件概率分布，损失函数为对数损失函数，经验风险最小化等价于极大似然估计。\n",
    "当样本容量很小时，经验风险最小化会产生过拟合现象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结构风险最小化(structural risk minimization,SRM)\n",
    "是为了防止过拟合提出的策略，结构风险最小化等价于正则化。\n",
    "\n",
    "结构风险在经验风险上加上正则化项（regularizer）或罚项（penalty term）（表示模型复杂度）。\n",
    "在假设空间，损失函数和训练数据集确定的情况下，结构风险定义为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  R_{srm} = \\frac{1}{N}\\sum_{i=1}^{N}{L(y_i,f(x_i))+\\lambda J(f)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(f)$为模型复杂度（模型f越复杂，越大）复杂度代表了对复杂模型的惩罚。$\\lambda$不小于零，是权衡经验风险和模型复杂度的系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 求解最优化模型的方法\n",
    " 一般有解析解，梯度下降（GD），随机梯度下降（SGD），牛顿法等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型评估和模型选择\n",
    "训练误差和测试误差\n",
    "当损失函数确定时，基于损失函数的训练误差和测试误差是评估标准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设学习模型为$Y = \\hat f(X)$，训练误差是模型关于训练数据集的平均损失\n",
    "$$  R_{emp}(\\hat f) = \\frac{1}{N}\\sum_{i=1}{N}{L(y_i,\\hat f(x))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试误差是模型$ Y=\\hat f(X)$关于测试数据集的平均损失\n",
    "$$  e_{test} = \\frac{1}{N'}\\sum_{i=1}^{N'}{L(y_i,\\hat f(x_i)}$$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一味追求对训练数据的预测能力，所选模型的复杂度往往比真模型更高，称为过拟合（over-fitting）\n",
    "指学习时选择的模型包含的参数过多，出现对已知数据预测得好，对未知数据预测的很差的情况\n",
    "\n",
    "当模型的复杂度增大时，训练误差会逐渐减小并趋向于0，而测试误差会先减小，达到最小值后增大。\n",
    "所以要进行最优的模型选择，已达到测试误差最小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择模型的方法\n",
    "正则化（regularization）\n",
    "典型方法，是结构风险最小化测策略的实现，在经验风险上加正则化项（regularizer）或罚项（penalty term）\n",
    "\n",
    "正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大\n",
    "一般具有如下形式：$$\\min_{f\\in F}\\frac{1}{N}\\sum_{i=1}^{N}{L(y_i,f(x_i))+\\lambda J(f)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一项为经验风险，第二项为正则化项，$\\lambda$为调整两者之间关系的系数\n",
    "\n",
    "正则化符合奥卡姆剃刀原理（Occam‘s razor），（Entities should not be multiplied unnecessarily）\n",
    "应用于模型选择，即在所有可能的模型中，能很好的解释已知数据并简单的是最好的模型。\n",
    "\n",
    "从贝叶斯估计的角度来看，正则化项对应模型的先验概率，可以假设复杂模型有较小的先验概率，简单的模型相反"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在给定数据集充足的情况下，模型选择的一种简单方法\n",
    "讲数据集切分为三个部分，训练集（training）、验证集（validation set)、和测试集（test set）\n",
    "在不同复杂度的模型中，选择对验证集有最小预测误差的模型。\n",
    "\n",
    "但在数据不足的情况下，可以采用交叉验证的方法，基本思想是重复的运用数据\n",
    "把给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复的操作\n",
    "\n",
    "1.简单交叉验证\n",
    "将一部分数据作为训练集，一部分作为测试集，用训练集在各种条件下测试模型（如不同参数），可得到不同的模型，在测试集上评价各个模型的测试误差\n",
    "\n",
    "2.S折交叉验证\n",
    "（S-fold cross validation）\n",
    "随机的将已给数据切分为S个互不相加，大小相同的子集，用S-1个子集训练模型，用剩下的子集测试，重复进行选择，选出S次评测中测试误差最小的模型。\n",
    "\n",
    "3.留一交叉验证\n",
    "S折交叉验证的特殊情况是S=N，称为留一交叉验证（leave-one-out cross validation），在数据缺乏的情况下使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "泛化能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "泛化误差（generalization ability）\n",
    "指学习到的模型对未知数据的预测能力\n",
    "如果学习到的模型是$\\hat f$，那么用这个模型对位置数据预测的误差即为泛化误差（generalization error）\n",
    "$$R_{exp}(\\hat f) = E_p[L(Y,\\hat f(X))]\\\\ = \\int_{x \\times y} L(y,\\hat f(x))P(x,y){\\rm d}x{\\rm d}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "泛化误差越小，表明这个模型越好。in fact，泛化误差就是学习到的模型的期望风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "泛化误差上界（泛化误差的概率上界）\n",
    "（generalization error bound）\n",
    "即通过比较两种学习方法的误差上界来比较他们的优劣\n",
    "它是样本容量的函数，样本容量增大时，泛化上界趋于0，也是假设空间容量（capacity）的函数，假设空间容量越大，泛化误差上界越大\n",
    "\n",
    "定理\n",
    "对二分类问题，当假设空间是有限个函数的集合$ F = \\{ f_1,f_2,\\cdot \\cdot \\cdot f_d\\}$时，对任意一个函数$f \\in F$，至少以概率$1-\\delta ,0< \\delta < 1$,有$$  R(f）\\leq \\hat R(f) + \\epsilon (d,N,\\delta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \\epsilon (d,N,\\delta) = \\sqrt{\\frac{1}{2N}(\\log d+\\log{\\frac{1}{\\delta}})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R(f)$是泛化误差，右端是泛化误差上界，$\\hat R(f）$是训练误差，训练误差越小，泛化误差上界越小，$\\epsilon(d,N,\\delta)$是N的单调递减函数，假设空间的函数越多，它的值越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成模型和判别模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习可以分为生成方法（generative approah）和判别方法（discriminative approach）\n",
    "所学的模型也称为生成模型（generative model）和判别模型（discriminative model）\n",
    "\n",
    "生成方法\n",
    "学习联合概率分布P（X,Y）,然后求出条件概率分布P（Y|X）作为生成模型\n",
    "$$  P(Y|X) = \\frac{P(X,Y)}{P(X)}$$\n",
    "典型的生成模型包括朴素贝叶斯和隐马尔科夫模型\n",
    "\n",
    "\n",
    "\n",
    "判别方法\n",
    "数据直接学习决策函数f（X）或条件概率分布P（Y|X）作为判别模型\n",
    "典型的包括k近邻，感知机，logestic回归，最大熵模型，支持向量机，提升方法和条件随机场。\n",
    "\n",
    "生成方法可以还原出联合概率分布P（X,Y），收敛速度更快，存在隐变量时，仍适用。\n",
    "判别方法直接学习条件概率或决策函数，学习的准确率更高，可以对数据进行各种程度的抽象，定义特征并使用，可以简化学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类问题\n",
    "监督学习从数据中学习一个分类模型或分类决策函数（分类器classifier），分类器对新的输入进行输出预测，称为分类（classification），可能的输出称为类别（class）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标注问题\n",
    "标注问题是分类问题的推广，又是结构预测的简单形式\n",
    "标注问题是输入一个观测序列，输出是一个标记序列或状态序列。目标在于学习一个模型，使它能对观测序列给出标记序列作为预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归问题\n",
    "用于预测输入变量和输出变量之间的关系\n",
    "回归模型表示从输入变量到输出变量之间的映射关系的函数，等价于函数拟合。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
